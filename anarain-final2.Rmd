---
title: "36-462 Final Exam Problem 2"
author: "Anirudh Narain (anarain)"
date: "due 11:59PM 5/13/2018"
output: pdf_document
---

```{r}
load("TrainData.Rdata")
trainY.df = data.frame(trainY)



pr.out <- prcomp(trainX, scale=TRUE, center = TRUE)
summary(pr.out)

# prin.out <- princomp(trainX)
# summary(prin.out)

pca.test <- pr.out$x[,1:92]
dim(pr.out$x)

biplot(pr.out, choices = 4:5, scale=0)
```

```{r}
hc.single <- hclust(dist(trainX), method = "single")
hc.complete <- hclust(dist(trainX), method = "complete")

plot(hc.single, main = "Single linkage", xlab = "", sub = "", cex = 0.5)
plot(hc.complete, main = "Complete linkage", xlab = "", sub = "", cex = 0.5)
```

```{r}
# Linear Discriminant Analysis

require(MASS)

n = nrow(trainX)
p = ncol(trainX)

a = lda(trainX, trainY)
at = a$scaling

trainX.mat = as.matrix(trainX)

z = trainX.mat %*% at

plot( z, col = trainY+1)
```

```{r}
# Fitting LDA to the data
lda.fit <- lda(grouping = trainY, x = trainX.mat)

# prior prob of group 0: 0.8346; prior prob of group 1: 0.1654
plot(lda.fit)

lda.pred <- predict(lda.fit, trainX)
table(lda.pred$class, trainY)


# testing
set.seed(1)
samp <- sample(1:133, 33)
train.lda <- trainX.mat[-samp,]
test.lda <- trainX[samp,]
lda.fit.1 <- lda(grouping = trainY[-samp], x = train.lda)

lda.predzz <- predict(lda.fit.1, newdata = test.lda)
table(lda.predzz$class, trainY[samp])

# Misclassification error:
min(sum(lda.predzz$class != trainY[samp])/length(trainY[samp]), 
                   sum(!lda.predzz$class != trainY[samp])/length(trainY[samp]))

library(combinat)

best_cluster <- function(cut1, cut2) {
  per <- permn(1:2)
  hamming <- numeric(length(per))
  for (i in 1:length(per)) {
    ts <- table(cut1, cut2)
    ts <- ts[per[[i]],]
    diag(ts) <- 0
    hamming[i] <- sum(ts)
  }
  return(c(min(hamming), which.min(hamming)))
}

best_cluster(lda.pred$class, trainY)
```

```{r}
# logistic regression

alldata <- cbind(trainX, trainY)
log.fit <- glm(trainY ~ ., data = alldata, family = binomial)
log.probs <- predict(log.fit, type = "response")
log.pred <- rep(0, 133)
log.pred[log.probs > 0.5] = 1

table(log.pred, trainY)

# testing
set.seed(1)
samp <- sample(1:133, 33)
train.alldata <- alldata[-samp,]
test.alldata <- alldata[samp,]
log.fit.1 <- glm(trainY~ ., data= train.alldata, family=binomial)
log.prob.1 <- predict(log.fit.1, newdata = test.alldata, type = "response")
log.predzz <- rep(0,33)
log.predzz[log.prob.1 > 0.5] = 1
table(log.predzz, test.alldata$trainY)

# Misclassification error:
min(sum(log.predzz != test.alldata$trainY)/length(test.alldata$trainY), 
                   sum(!log.predzz != test.alldata$trainY)/length(test.alldata$trainY))
```

```{r}
# K-nn regression
library(class)
samp <- sample(1:133, 33)
train.knn <- trainX[-samp,]
test.knn <- trainX[samp,]

knn.pred <- knn(train.knn, test.knn, trainY[-samp], k = 7)

table(knn.pred, trainY[samp])

min(sum(knn.pred != trainY[samp])/length(trainY[samp]), 
                   sum(!knn.pred != trainY[samp])/length(trainY[samp]))
```

Supervised learning: For this problem, we have the class labels associated with the training data available to us, and we attempt to find a supervised learning approach that yields the best results (i.e. has the lowest Hamming classification error). The approaches that we chose to investigate were linear discriminant analysis (LDA), logistic regression, linear regression, ridge regression, the lasso, k-nearest neighbors (k-NN), and decision trees.

