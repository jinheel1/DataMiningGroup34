---
title: "Final Project Report from Group 34"
author: "Maria Rodriguez (mar1); Anirudh Narain (anarain); Kayla Oliva (keoliva); Jinhee Lee (jinheel1)"
output: 
  pdf_document: 
    highlight: tango
    latex_engine: xelatex
    df_print: kable
    keep_tex: yes
    fig_crop: yes
mainfont: Palatino
fontsize: 11pt
---
```{r global_options, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, cache=T, autodep=F, cache.comments=FALSE, 
                      warning=F, message=F, tidy=T, strip.white = TRUE)
```

# Unsupervised Learning

## Q1a: Is there any interesting structure present in the data? 

The data frame given to us contained 133 patients with 248 gene encodings. Given this high dimensionality it is extremely difficult to determine if there is any structure in the data. To solve this we explore several different dimension reduction methods to determine what sort of structure exists in the data. 

The first method explored is Principal Component Analysis (PCA), PCA is used as a technique to emphasize variation and highllight strong patterns in data. This also makes it easy to explore and visualize. [Source](http://setosa.io/ev/principal-component-analysis/) 

The two methods that apply PCA in R are princomp and prcomp. Princomp cannot be used on X because it uses the spectral decomposition approach, this would factorize X into a canonical form where it is represented by its eigenvalues and eigenvectors. Only diagnolizable matrices can be factored in this way, X is not a diagnolizable matrix.
[Source1](http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/118-principal-component-analysis-in-r-prcomp-vs-princomp/)
[Source2](https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix)
[Source3](https://en.wikipedia.org/wiki/Diagonalizable_matrix)

Thus we apply prcomp to our data because it uses singular value decomposition, this examines the covariance and correlations between our observed patients and doesn't require X to be a diagnolizable matrix. But before this we explore it for a bit.

Load Data, Explore type, shape and NAs
```{r}
X <- read.table(file="trainX.txt",header=TRUE)
y <- read.table(file="trainy.txt",header=TRUE)

#Shape
dim(X)
#Checking to see if any column is fully 0, there's none
sum(colSums(X) == 0)
#No NA's
sum(apply(X, 1, function(x){any(is.na(x))}))

#These have really long output so we're not executing them 
#but it seems all column's are doubles
#summary(X)
#str(X)
head(X[,1:5])
```

Feature Selection
```{r}
#install.packages("caret", dependencies = TRUE)
library(ggplot2)
library(caret)

correlationMatrix <- cor(X)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.85)
print(highlyCorrelated)
```


PCA: With prcomp we consider the case where X it is scaled. From our summary output we see that they have many different ranges and these large differences in magnitude make scaling an appropriate step to take in this scenario. If we weren't to scale our variables the magnitude of certain variables would dominate to associations between variables in the sample.(Source)[https://stats.stackexchange.com/questions/268264/normalizing-all-the-variarbles-vs-using-scale-true-option-in-prcomp-in-r].
```{r}
pc.n = prcomp(X, center = T, scale = T)
names(pc.n)
summary(pc.n)
#PC1 explains 11%
#PC50 explains 90%
#PC65 explains 95%
#PC92 explains 99%
```

To visualize the dimensions we use factoextra to create a ggplot2 visualization of our PCA objects.
```{r}
install.packages("ggplot2")
install.packages("factoextra")
library(ggplot2)
library(factoextra)
```

The screeplot of PC object shows an elbow around 10 but that isn't enough variance explained. To determine the amount of PC's to retain I need to see the increasing gains in variance explained by each new PC. Using this it seems that the elbow of cumulative variance explained is passed around 60. So going down to 65 PC's seems to be the reasonable given it covers 95% of variance. The proportion of variance explained by the principal component directions is a way of quanitfying how much structure of the data is being captured and if we can capture 95% of the data from 65 PC's that's much better than 100% of the data being captured from 248 variables.
```{r}
fviz_eig(pc.n, ncp = 80)
vars = apply(pc.n$x, 2, var)
prop_cumsum = cumsum(vars / sum(vars))
qplot(seq_along(prop_cumsum), prop_cumsum) + xlab("PC") + 
  ylab("Cumulative Proportion of Variance Explained") + theme_minimal() +
  scale_x_continuous(breaks = seq(0, 133, by = 5)) +
  scale_y_continuous(breaks = seq(0, 1, by = 0.05))
#plot(diff(prop_cumsum))
```

You can observe that there are two groups of variables that have similar meaning in the context of the data. The variables group on the right seem to have large positive loadings on dimension 2. On the group that is pointing left they seem to have high negative loadings on dimension 1.
There's also the variables clustered in the center lower right half, these are pointing away from the two extreme clusters which means they are unlike those two groups, they have low positives loadings on dimension 1 and dimension 2.
Looking at the contributions of variables it seems agreed upon in the variable loading plot that there's three different variable behaviors, those positive in dim2, negative on dim1, and all others.
```{r}
fviz_pca_biplot(pc.n)
fviz_pca_var(pc.n,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE)

```

For the patients themselves it seems there are two differing groups, one highly positive on dimension 2 and one highly negative on dimension 1 and these groups have high contributions to the data. Most other patients lie in the center of the two dimensions.
```{r}
fviz_pca_ind(pc.n,
             col.ind = "contrib", # Color by the quality of representation
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE)
```

So settling on 65PC's we apply our clustering methods to see which works best on our data.
```{r}
pc.65 = pc.n$x[,1:65]
```

## Clustering Methods Used

#### Hierarchical Clustering
One of the methods we used for clustering was hierarchical clustering. We tried four different types of linkage, and observed that the complete and average linkage had the lowest hamming clustering errors. The classification errors are presented below, including before and after applying normalized PCA to the training data. Although, single linkage suffers from chaining, complete linkage suffers from crowding, and average linkage strikes a balance so that clusters are simulataneously relatively compact and far away, the clustering results of complete and average linkage ended up doing better than single and centroid linkages' results. Since, average linkage results in more ideal clusters, we will choose the result of average linkage hierarchical clustering, to compare against the rest of the unsupervised learning models. 
```{r}
set.seed(1)
# install.packages("clues", dependencies = T)
# install.packages("combinat", dependencies = T)
# install.packages("dendextend", dependencies = T)
# install.packages("pander", dependencies = T)
library(clues)
library(combinat)
library(dendextend)
library(pander)

compute.clustering.error <- function(true.labels, predictions) {
  perms <- permn(c(0, 1))
  min.error <- Inf
  for (p in 1:length(perms)) {
    pi <- perms[[p]]
    error <- 0
    for (i in 1:length(true.labels)) {
      l <- true.labels[i]
      l.pred <- predictions[i]
      if (l != pi[l.pred]) {
        error <- error + 1
      }
    }
    if (error < min.error) {
      min.error <- error
    }
  }
  return(min.error)
}

methods <- c("Single", "Complete", "Average", "Centroid")
n <- nrow(X)
clusterings <- function(withPCA, res, makePlot=FALSE) {
  if (withPCA) {
    resRow <- 2
    pr.out <- prcomp(X, center=TRUE, scale=TRUE)
    dist.matrix <- dist(pr.out$x[,1:65])
    title <- "After"
  } else {
    resRow <- 1
    dist.matrix <- dist(X)
    title <- "Before"
  }
  for (i in 1:4) {
    method <- methods[i]
    hc.out <- hclust(dist.matrix, method=tolower(method))
    hc.clustering <- cutree(hc.out, 2)
    res[resRow, i] <- compute.clustering.error(y$label, (hc.clustering)) / n
    if (makePlot && (method == "Single" || method == "Complete")) {
      dend <- hc.out %>% as.dendrogram %>% set("labels_cex", .35) %>% color_branches(k = 2)
      plot(dend, main=paste(method, " Linkage", "\n(", title, " Applying PCA)", sep=""))
    }
  }
  res
}

res <- clusterings(TRUE, clusterings(FALSE, matrix(NA, nrow=2, ncol=4)))
colnames(res) <- methods
rownames(res) <- c("Before Applying PCA", "After Applying PCA")
pander(res) #Hamming Cluster Errors
```

```{r}
# c(bottom, left, top, right) 
par(mfrow=c(2, 2), mar=c(2,5,2,0.5))
res <- clusterings(TRUE, clusterings(FALSE, matrix(NA, nrow=2, ncol=4), TRUE), TRUE)
```

#### K-Means

An unsupervised learning method we apply is K-Means to determine cluster groupings in our data. But before we can determine the final cluster's it's helpful to determine the best K. 
```{r}
library(cluster)
library(factoextra)
set.seed(1)
#It seems K = 2 is the best
fviz_nbclust(X, kmeans, method = "silhouette") + theme_classic()
fviz_nbclust(pc.65, kmeans, method = "silhouette") + theme_classic()

kmeans.results = kmeans(X, centers = 2, nstart = 25, iter.max = 20, algorithm = "Lloyd")
kmeans.results.pca = kmeans(pc.65, centers = 2, nstart = 25, iter.max = 20, algorithm = "Lloyd")

best_cluster <- function(cut1, cut2) {
  per <- permn(1:2)
  hamming <- numeric(length(per))
  for (i in 1:length(per)) {
    ts <- table(cut1, cut2)
    ts <- ts[per[[i]],]
    diag(ts) <- 0
    hamming[i] <- sum(ts)
  }
  return(c(min(hamming), which.min(hamming), min(hamming)/length(cut2)))
}
num_patients = nrow(X)
err_calc = best_cluster(kmeans.results$cluster, y$label)
err_calc[3] #Error
err_calc.pca = best_cluster(kmeans.results.pca$cluster, y$label)
err_calc.pca[3] #Error
```

#### K-Medoids

Another method we used for clustering was K-medoids clustering. Below, the code calculates the optimal number of clusters to be 2, clusters the data using the `pam()` function, and visualizaes the data. We used a custom function `best_cluster()` and calculated the error in the clustering to be about 10.5%. Although more computationally difficult, k-medoids clustering...


```{r}
library(factoextra)
library(cluster)
# Calculating optimal number of clusters
fviz_nbclust(X, pam, method = "silhouette") + theme_classic()
fviz_nbclust(pc.65, pam, method = "silhouette") + theme_classic()
```

```{r}
pam.res = pam(X, k = 2)
pam.res.65.2 = pam(pc.65, k = 2)
pam.res.65.4 = pam(pc.65, k = 4)

# Error proportion: the number of incorrect classification divided the number of rows
kmed_error = best_cluster(y$label, pam.res$clustering)[3]
kmed_error.65.2 = best_cluster(y$label, pam.res.65.2$clustering)[3]
kmed_error.65.4 = best_cluster(y$label, pam.res.65.4$clustering)[3]

cat("Error:", round(kmed_error * 100, 2), "%")
cat("Error:", round(kmed_error.64.2 * 100, 2), "%")
cat("Error:", round(kmed_error.64.4 * 100, 2), "%")
```


# Supervised Learning

## Methods Used

### LDA

```{r}
# Linear Discriminant Analysis
load("TrainData.Rdata")
trainY.df = data.frame(trainY)

require(MASS)

n = nrow(trainX)
p = ncol(trainX)

a = lda(trainX, trainY)
at = a$scaling

trainX.mat = as.matrix(trainX)

z = trainX.mat %*% at

plot( z, col = trainY+1)
```

```{r}
# Fitting LDA to the data
lda.fit <- lda(grouping = trainY, x = trainX.mat)

# prior prob of group 0: 0.8346; prior prob of group 1: 0.1654
plot(lda.fit)

lda.pred <- predict(lda.fit, trainX)
table(lda.pred$class, trainY)


# testing
set.seed(1)
samp <- sample(1:133, 33)
train.lda <- trainX.mat[-samp,]
test.lda <- trainX[samp,]
lda.fit.1 <- lda(grouping = trainY[-samp], x = train.lda)

lda.predzz <- predict(lda.fit.1, newdata = test.lda)
table(lda.predzz$class, trainY[samp])

# Misclassification error:
min(sum(lda.predzz$class != trainY[samp])/length(trainY[samp]), 
                   sum(!lda.predzz$class != trainY[samp])/length(trainY[samp]))

library(combinat)

best_cluster <- function(cut1, cut2) {
  per <- permn(1:2)
  hamming <- numeric(length(per))
  for (i in 1:length(per)) {
    ts <- table(cut1, cut2)
    ts <- ts[per[[i]],]
    diag(ts) <- 0
    hamming[i] <- sum(ts)
  }
  return(c(min(hamming), which.min(hamming)))
}

best_cluster(lda.pred$class, trainY)
```

###Logistic Regression


```{r}
# logistic regression

alldata <- cbind(trainX, trainY)
log.fit <- glm(trainY ~ ., data = alldata, family = binomial)
log.probs <- predict(log.fit, type = "response")
log.pred <- rep(0, 133)
log.pred[log.probs > 0.5] = 1

table(log.pred, trainY)

# testing
set.seed(1)
samp <- sample(1:133, 33)
train.alldata <- alldata[-samp,]
test.alldata <- alldata[samp,]
log.fit.1 <- glm(trainY~ ., data= train.alldata, family=binomial)
log.prob.1 <- predict(log.fit.1, newdata = test.alldata, type = "response")
log.predzz <- rep(0,33)
log.predzz[log.prob.1 > 0.5] = 1
table(log.predzz, test.alldata$trainY)

# Misclassification error:
min(sum(log.predzz != test.alldata$trainY)/length(test.alldata$trainY), 
                   sum(!log.predzz != test.alldata$trainY)/length(test.alldata$trainY))
```

### kNN
```{r}
# K-nn regression
library(class)
samp <- sample(1:133, 33)
train.knn <- trainX[-samp,]
test.knn <- trainX[samp,]

#tune k
knn.pred <- knn(train.knn, test.knn, trainY[-samp], k = 7)

table(knn.pred, trainY[samp])

min(sum(knn.pred != trainY[samp])/length(trainY[samp]), 
                   sum(!knn.pred != trainY[samp])/length(trainY[samp]))
```


### Trees

```{r}
library(randomForest)
data = as.data.frame(cbind(y,X))
bag.data = randomForest(as.factor(label) ~ ., data, mtry = 20, importance = T)

bag.data
```