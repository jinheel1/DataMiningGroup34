---
title: "Final Project Report from Group 34"
author: "Maria Rodriguez (mar1); Anirudh Narain (anarain); Kayla Oliva (keoliva); Jinhee Lee (jinheel1)"
output: 
  pdf_document: 
    highlight: tango
    latex_engine: xelatex
    df_print: kable
    keep_tex: yes
    fig_crop: yes
mainfont: Palatino
fontsize: 11pt
---
```{r global_options, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, cache=T, autodep=F, cache.comments=FALSE, 
                      warning=F, message=F, tidy=T, strip.white = TRUE)
```
# Unsupervised Learning

## Is there any interesting structure present in the data? 

## Clustering Methods Used

#### Hierarchical Clustering
One of the methods we used for clustering was hierarchical clustering. We tried four different types of linkage, and observed that the complete and average linkage had the lowest hamming clustering errors. The classification errors are presented below, including before and after applying normalized PCA to the training data. Although, single linkage suffers from chaining, complete linkage suffers from crowding, and average linkage strikes a balance so that clusters are simulataneously relatively compact and far away, the clustering results of complete and average linkage ended up doing better than single and centroid linkages' results. Since, average linkage results in more ideal clusters, we will choose the result of average linkage hierarchical clustering, to compare against the rest of the unsupervised learning models. 

```{r, message=F, warning=F, width=10, height=8, echo=F}
load("TrainData.RData")
set.seed(1)
library(clues)
library(combinat)
library(dendextend)
library(pander)

compute.clustering.error <- function(true.labels, predictions) {
  perms <- permn(c(0, 1))
  min.error <- Inf
  for (p in 1:length(perms)) {
    pi <- perms[[p]]
    error <- 0
    for (i in 1:length(true.labels)) {
      l <- true.labels[i]
      l.pred <- predictions[i]
      if (l != pi[l.pred]) {
        error <- error + 1
      }
    }
    if (error < min.error) {
      min.error <- error
    }
  }
  return(min.error)
}

methods <- c("Single", "Complete", "Average", "Centroid")
n <- nrow(trainX)
clusterings <- function(withPCA, res, makePlot=FALSE) {
  if (withPCA) {
    resRow <- 2
    pr.out <- prcomp(trainX, center=TRUE, scale=TRUE)
    dist.matrix <- dist(pr.out$x[,1:2])
    title <- "After"
  } else {
    resRow <- 1
    dist.matrix <- dist(trainX)
    title <- "Before"
  }
  for (i in 1:4) {
    method <- methods[i]
    hc.out <- hclust(dist.matrix, method=tolower(method))
    hc.clustering <- cutree(hc.out, 2)
    res[resRow, i] <- compute.clustering.error(trainY, (hc.clustering)) / n
    if (makePlot && (method == "Single" || method == "Complete")) {
      dend <- hc.out %>% as.dendrogram %>% set("labels_cex", .35) %>% color_branches(k = 2)
      plot(dend, main=paste(method, " Linkage", "\n(", title, " Applying PCA)", sep=""))
    }
  }
  res
}

res <- clusterings(TRUE, clusterings(FALSE, matrix(NA, nrow=2, ncol=4)))
colnames(res) <- methods
rownames(res) <- c("Before Applying PCA", "After Applying PCA")
pander(res)
```

```{r, echo=F}
# c(bottom, left, top, right) 
par(mfrow=c(2, 2), mar=c(2,5,2,0.5))
res <- clusterings(TRUE, clusterings(FALSE, matrix(NA, nrow=2, ncol=4), TRUE), TRUE)
```

#### K-Means

#### K-Medoids

Another method we used for clustering was K-medoids clustering. Below, the code calculates the optimal number of clusters to be 2, clusters the data using the `pam()` function, and visualizaes the data. We used a custom function `best_cluster()` and calculated the error in the clustering to be about 10.5%. Although more computationally difficult, k-medoids clustering...

```{r, echo=F}
library(factoextra)
library(cluster)
# Calculating optimal number of clusters
fviz_nbclust(trainX, pam, method = "silhouette") + theme_classic()


pam.res = pam(trainX, k = 2)

# Visualization of K-medoids clustering
fviz_cluster(pam.res, 
             palette = c("#00AFBB", "#FC4E07"), # color palette
             ellipse.type = "t", # Concentration ellipse
             repel = TRUE, # Avoid label overplotting (slow)
             ggtheme = theme_classic()
             )
```

```{r, echo=F}
library(ClusterR)

# Error proportion: the number of incorrect classification divided the number of rows
kmed_error = best_cluster(trainY, pam.res$clustering)[1] / nrow(trainX)
cat("Error:", kmed_error * 100, "%")
```



# Supervised Learning

## Methods Used

#### LDA

#### QDA

#### kNN

#### Decision Trees
